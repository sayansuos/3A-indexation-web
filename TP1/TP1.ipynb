{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aaebb48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import urllib.robotparser\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from collections import deque\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35e38597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_html_all(url:str):\n",
    "    '''\n",
    "    Prints all the text from a page with a given url.\n",
    "    '''\n",
    "    with urllib.request.urlopen(url) as f:\n",
    "        html_doc = f.read()\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "    print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72dfc95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_components(url:str) -> dict:\n",
    "    '''\n",
    "    Extracts all the components needed (title, description and links) from a page with a given url.\n",
    "    '''\n",
    "    with urllib.request.urlopen(url) as f:\n",
    "        html_doc = f.read()\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "    title = soup.title\n",
    "    if title is not None:\n",
    "        title = title.string\n",
    "    else:\n",
    "        title = \"\"\n",
    "\n",
    "    description = soup.find('p', class_='product-description')\n",
    "    if description is not None:\n",
    "        description = description.get_text()\n",
    "    else:\n",
    "        description = \"\"\n",
    "\n",
    "    links = []\n",
    "    all_links = soup.find_all(href=re.compile(r'^http')) # To only keep real links\n",
    "    if all_links is not None:\n",
    "        for link in all_links:\n",
    "            links.append(link.get('href'))\n",
    "\n",
    "    output = {\n",
    "        'url': url,\n",
    "        'title': title,\n",
    "        'description': description,\n",
    "        'links': links,\n",
    "    }\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2411eb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_robots_url(url:str) -> str:\n",
    "    '''\n",
    "    Gives the robots.txt's url from a page with a given url.\n",
    "    '''\n",
    "    o = urllib.parse.urlparse(url)\n",
    "    url_robot = f\"{o.scheme}://{o.netloc}/robots.txt\"\n",
    "    return url_robot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a957466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_allowed(url:str, rp:urllib.robotparser.RobotFileParser, useragent:str='*',) -> bool:\n",
    "    '''\n",
    "    Returns True if a page with a given url can be parsed by the crawler, False otherwise.\n",
    "    '''\n",
    "    return rp.can_fetch(useragent, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "feb2eaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_queue(queue:deque, visited:set, new_urls:list, rp:urllib.robotparser.RobotFileParser) -> deque:\n",
    "    '''\n",
    "    Gives the updated queue and visited set.\n",
    "    '''\n",
    "    for url in new_urls:\n",
    "        if is_allowed(url, rp) and not url in visited: # We check if the page can be parsed and if it has not already been\n",
    "            o = urllib.parse.urlparse(url)\n",
    "            if \"product\" in o.path: # To priorize pages with 'product' token\n",
    "                queue.appendleft(url)\n",
    "            else:\n",
    "                queue.append(url)\n",
    "    return queue \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca6aebc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawler(url:str, path_out:str, n_max:int=50):\n",
    "    '''\n",
    "    Crawls a number of pages starting from a given url.\n",
    "    '''\n",
    "    i = 0\n",
    "    data = []\n",
    "    queue = deque([url])\n",
    "    visited = set()\n",
    "\n",
    "    robot_url = get_robots_url(url)\n",
    "    rp = urllib.robotparser.RobotFileParser()\n",
    "    rp.set_url(robot_url)\n",
    "    rp.read()\n",
    "\n",
    "    with open(path_out, 'w', encoding='utf-8') as f:\n",
    "        while i < n_max and queue: # We only want to extract n_max pages\n",
    "            current_url = queue.popleft() # We remove the first url from the queue\n",
    "            if current_url in visited: # If the page has already been crawled, we skip it\n",
    "                continue\n",
    "\n",
    "            visited.add(current_url)\n",
    "            new = get_html_components(current_url) # We get the data we need from the page\n",
    "            ligne = json.dumps(new, ensure_ascii=False)\n",
    "            f.write(ligne + \"\\n\")\n",
    "            init = i == 0 \n",
    "            queue = update_queue(queue, visited, new['links'], rp)\n",
    "            time.sleep(0.5) # To avoid overload\n",
    "            print(f\"Page {i+1}/{n_max} has been crawled!\")\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "756aa7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1/50 has been crawled!\n",
      "Page 2/50 has been crawled!\n",
      "Page 3/50 has been crawled!\n",
      "Page 4/50 has been crawled!\n",
      "Page 5/50 has been crawled!\n",
      "Page 6/50 has been crawled!\n",
      "Page 7/50 has been crawled!\n",
      "Page 8/50 has been crawled!\n",
      "Page 9/50 has been crawled!\n",
      "Page 10/50 has been crawled!\n",
      "Page 11/50 has been crawled!\n",
      "Page 12/50 has been crawled!\n",
      "Page 13/50 has been crawled!\n",
      "Page 14/50 has been crawled!\n",
      "Page 15/50 has been crawled!\n",
      "Page 16/50 has been crawled!\n",
      "Page 17/50 has been crawled!\n",
      "Page 18/50 has been crawled!\n",
      "Page 19/50 has been crawled!\n",
      "Page 20/50 has been crawled!\n",
      "Page 21/50 has been crawled!\n",
      "Page 22/50 has been crawled!\n",
      "Page 23/50 has been crawled!\n",
      "Page 24/50 has been crawled!\n",
      "Page 25/50 has been crawled!\n",
      "Page 26/50 has been crawled!\n",
      "Page 27/50 has been crawled!\n",
      "Page 28/50 has been crawled!\n",
      "Page 29/50 has been crawled!\n",
      "Page 30/50 has been crawled!\n",
      "Page 31/50 has been crawled!\n",
      "Page 32/50 has been crawled!\n",
      "Page 33/50 has been crawled!\n",
      "Page 34/50 has been crawled!\n",
      "Page 35/50 has been crawled!\n",
      "Page 36/50 has been crawled!\n",
      "Page 37/50 has been crawled!\n",
      "Page 38/50 has been crawled!\n",
      "Page 39/50 has been crawled!\n",
      "Page 40/50 has been crawled!\n",
      "Page 41/50 has been crawled!\n",
      "Page 42/50 has been crawled!\n",
      "Page 43/50 has been crawled!\n",
      "Page 44/50 has been crawled!\n",
      "Page 45/50 has been crawled!\n",
      "Page 46/50 has been crawled!\n",
      "Page 47/50 has been crawled!\n",
      "Page 48/50 has been crawled!\n",
      "Page 49/50 has been crawled!\n",
      "Page 50/50 has been crawled!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://web-scraping.dev/products\"\n",
    "path_out = \"output/products.jsonl\"\n",
    "n_max = 50\n",
    "crawler(url, path_out, n_max)\n",
    "\n",
    "with open(path_out, 'r', encoding='utf-8') as f:\n",
    "    data = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "data.sort(key=lambda x: x.get('url', ''))\n",
    "\n",
    "with open(path_out, 'w', encoding='utf-8') as f:\n",
    "    f.writelines(json.dumps(d, ensure_ascii=False) + '\\n' for d in data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
