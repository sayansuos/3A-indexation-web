{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "85f903ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /Users/sayansuos/Library/CloudStorage/OneDrive-Personnel/Documents/ENSAI/3A/Indexation web/3A-indexation-web/.venv/lib/python3.11/site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/sayansuos/Library/CloudStorage/OneDrive-Personnel/Documents/ENSAI/3A/Indexation web/3A-indexation-web/.venv/lib/python3.11/site-packages (from bs4) (4.14.3)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in /Users/sayansuos/Library/CloudStorage/OneDrive-Personnel/Documents/ENSAI/3A/Indexation web/3A-indexation-web/.venv/lib/python3.11/site-packages (from beautifulsoup4->bs4) (2.8.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/sayansuos/Library/CloudStorage/OneDrive-Personnel/Documents/ENSAI/3A/Indexation web/3A-indexation-web/.venv/lib/python3.11/site-packages (from beautifulsoup4->bs4) (4.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "aaebb48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from collections import deque\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "35e38597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_html_all(url:str):\n",
    "    '''\n",
    "    Prints all the text from a page with a given url.\n",
    "    '''\n",
    "    with urllib.request.urlopen(url) as f:\n",
    "        html_doc = f.read()\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "    print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dfc95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_components(url:str) -> dict:\n",
    "    '''\n",
    "    Extracts all the components needed (title, description and links) from a page with a given url.\n",
    "    '''\n",
    "    with urllib.request.urlopen(url) as f:\n",
    "        html_doc = f.read()\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "    title = soup.title\n",
    "    if title is not None:\n",
    "        title = title.string\n",
    "    else:\n",
    "        title = \"\"\n",
    "\n",
    "    description = soup.find('p', class_='product-description')\n",
    "    if description is not None:\n",
    "        description = description.get_text()\n",
    "    else:\n",
    "        description = \"\"\n",
    "\n",
    "    links = []\n",
    "    all_links = soup.find_all(href=re.compile(r'^http')) # To only keep real links\n",
    "    if all_links is not None:\n",
    "        for link in all_links:\n",
    "            links.append(link.get('href'))\n",
    "\n",
    "    output = {\n",
    "        'url': url,\n",
    "        'title': title,\n",
    "        'description': description,\n",
    "        'links': links,\n",
    "    }\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "2411eb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_robots_url(url:str) -> str:\n",
    "    '''\n",
    "    Gives the robots.txt's url from a page with a given url.\n",
    "    '''\n",
    "    o = urllib.parse.urlparse(url)\n",
    "    url_robot = f\"{o.scheme}://{o.netloc}/robots.txt\"\n",
    "    return url_robot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "1a957466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def can_beparsed(url:str, rp:urllib.robotparser.RobotFileParser, useragent:str='*',) -> bool:\n",
    "    '''\n",
    "    Returns True if a page with a given url can be parsed by the crawler, False otherwise.\n",
    "    '''\n",
    "    return rp.can_fetch(useragent, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb2eaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_queue(queue:deque, visited:set, new_urls:list, init:bool, rp:urllib.robotparser.RobotFileParser) -> deque:\n",
    "    '''\n",
    "    Gives the updated queue and visited set.\n",
    "    '''\n",
    "    if not init: \n",
    "        old = queue.popleft() \n",
    "        visited.add(old)\n",
    "    for url in new_urls:\n",
    "        if can_beparsed(url, rp) and not url in visited: # We check if the page can be parsed and if it has not already been\n",
    "            o = urllib.parse.urlparse(url)\n",
    "            if \"product\" in o.path: # To priorize pages with 'product' token\n",
    "                queue.appendleft(url)\n",
    "            else:\n",
    "                queue.append(url)\n",
    "    return queue, visited \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6aebc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawler(url:str, n_max:int=50) -> list:\n",
    "    '''\n",
    "    Crawls a number of pages starting from a given url.\n",
    "    '''\n",
    "    i = 0\n",
    "    data = []\n",
    "    queue = deque([url])\n",
    "    visited = set()\n",
    "\n",
    "    robot_url = get_robots_url(url)\n",
    "    rp = urllib.robotparser.RobotFileParser()\n",
    "    rp.set_url(robot_url)\n",
    "    rp.read()\n",
    "\n",
    "    while i < n_max: # We only want to extract n_max pages\n",
    "        current_url = queue.popleft() # We remove the first url from the queue\n",
    "        if current_url in visited: # If the page has already been crawled, we skip it\n",
    "            continue\n",
    "\n",
    "        new = get_html_components(current_url) # We get the data we need from the page\n",
    "        data.append(new)\n",
    "        init = i == 0 \n",
    "        queue, visited = update_queue(queue, visited, new['links'], init, rp)\n",
    "        time.sleep(0.5) # To avoid overload\n",
    "        i += 1\n",
    "    \n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "756aa7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://web-scraping.dev/products\"\n",
    "n_max = 50\n",
    "data = crawler(url, n_max)\n",
    "data = sorted(data, key=lambda x: x['url'])\n",
    "\n",
    "with open('../output/products.json', 'w') as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
